{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvNj4apfO1wZKuRYu0xJ9X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: MNIST: http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "Training: 60,000\n",
        "Testing: 10,000\n",
        "\n",
        "Problem: Classify handwritten digits (0-9) using the MNIST dataset. This is a classification problem where each image is labeled with a digit.\n",
        "\n",
        "Dataset Source:\n",
        "http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "Dataset Overview:\n",
        "\n",
        "70,000 images (60,000 for training, 10,000 for testing)\n",
        "28×28 grayscale images\n",
        "10 digit classes (0-9)\n",
        "Complexity:\n",
        "Large dataset (70,000 samples)\n",
        "784 features per image (28×28 pixels)\n",
        "High variability in handwriting styles"
      ],
      "metadata": {
        "id": "T4PJthnVLQVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "ZawluKo00YhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"Initialize the network and its parameters.\"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.parameters = self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights with a small random value and biases with zeros.\"\"\"\n",
        "        np.random.seed(42)\n",
        "        params = {\n",
        "            'W1': np.random.randn(self.hidden_dim, self.input_dim) * 0.01,\n",
        "            'b1': np.zeros((self.hidden_dim, 1)),\n",
        "            'W2': np.random.randn(self.output_dim, self.hidden_dim) * 0.01,\n",
        "            'b2': np.zeros((self.output_dim, 1))\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def _relu(self, z):\n",
        "        \"\"\"ReLU activation function.\"\"\"\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        \"\"\"Softmax activation function for multiclass classification.\"\"\"\n",
        "        exp_values = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform forward propagation.\n",
        "        Returns the output probabilities and caches intermediate values.\n",
        "        \"\"\"\n",
        "        W1, b1 = self.parameters['W1'], self.parameters['b1']\n",
        "        W2, b2 = self.parameters['W2'], self.parameters['b2']\n",
        "\n",
        "        z1 = np.dot(W1, X) + b1\n",
        "        a1 = self._relu(z1)\n",
        "        z2 = np.dot(W2, a1) + b2\n",
        "        a2 = self._softmax(z2)\n",
        "\n",
        "        # Cache intermediate variables for backpropagation\n",
        "        self.cache = {'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n",
        "        return a2\n",
        "\n",
        "    def compute_cost(self, predictions, Y):\n",
        "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
        "        m = Y.shape[1]\n",
        "        # Added a small epsilon for numerical stability\n",
        "        cost = -np.sum(Y * np.log(predictions + 1e-8)) / m\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def _relu_derivative(self, z):\n",
        "        \"\"\"Compute derivative of the ReLU function.\"\"\"\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    def backward(self, X, Y):\n",
        "        \"\"\"\n",
        "        Perform backward propagation to compute gradients.\n",
        "        \"\"\"\n",
        "        m = X.shape[1]\n",
        "        W2 = self.parameters['W2']\n",
        "        z1, a1, a2 = self.cache['z1'], self.cache['a1'], self.cache['a2']\n",
        "\n",
        "        dz2 = a2 - Y  # derivative for softmax with cross-entropy\n",
        "        dW2 = np.dot(dz2, a1.T) / m\n",
        "        db2 = np.sum(dz2, axis=1, keepdims=True) / m\n",
        "\n",
        "        dz1 = np.dot(W2.T, dz2) * self._relu_derivative(z1)\n",
        "        dW1 = np.dot(dz1, X.T) / m\n",
        "        db1 = np.sum(dz1, axis=1, keepdims=True) / m\n",
        "\n",
        "        self.gradients = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
        "\n",
        "    def update_parameters(self, learning_rate):\n",
        "        \"\"\"Update network parameters using computed gradients.\"\"\"\n",
        "        for key in self.parameters:\n",
        "            self.parameters[key] -= learning_rate * self.gradients['d' + key]\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return the predicted classes for input X.\"\"\"\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=0)\n",
        "\n",
        "    def train(self, X, Y, iterations=1000, learning_rate=0.01, verbose=False):\n",
        "        \"\"\"\n",
        "        Train the neural network using traditional gradient descent.\n",
        "        \"\"\"\n",
        "        for i in range(iterations):\n",
        "            probs = self.forward(X)\n",
        "            cost = self.compute_cost(probs, Y)\n",
        "            self.backward(X, Y)\n",
        "            self.update_parameters(learning_rate)\n",
        "            if verbose and i % 100 == 0:\n",
        "                print(f\"Iteration {i}: cost = {cost:.4f}\")\n",
        "\n",
        "    def train_minibatch(self, X, Y, batch_size=32, iterations=1000, learning_rate=0.01, verbose=False):\n",
        "        \"\"\"\n",
        "        Train the neural network using minibatch gradient descent.\n",
        "        \"\"\"\n",
        "        m = X.shape[1]\n",
        "        for i in range(iterations):\n",
        "            # Shuffle the training data each iteration\n",
        "            indices = np.random.permutation(m)\n",
        "            X_shuffled = X[:, indices]\n",
        "            Y_shuffled = Y[:, indices]\n",
        "\n",
        "            for j in range(0, m, batch_size):\n",
        "                X_batch = X_shuffled[:, j:j+batch_size]\n",
        "                Y_batch = Y_shuffled[:, j:j+batch_size]\n",
        "\n",
        "                probs = self.forward(X_batch)\n",
        "                self.backward(X_batch, Y_batch)\n",
        "                self.update_parameters(learning_rate)\n",
        "            if verbose and i % 100 == 0:\n",
        "                batch_cost = self.compute_cost(probs, Y_batch)\n",
        "                print(f\"Minibatch Iteration {i}: cost = {batch_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "IEyhUvnrKjLE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- Data Preparation and Testing -------------------------\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V8IHtxpXMUO",
        "outputId": "9ab96f19-60da-4ab9-ce23-6c37df642bb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values to [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_test  = X_test  / 255.0\n",
        "\n",
        "# Flatten images: each column is one flattened image\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1).T  # shape: (784, number of examples)\n",
        "X_test_flat  = X_test.reshape(X_test.shape[0], -1).T\n",
        "\n",
        "# One-hot encode the labels and transpose so that each column is one label vector\n",
        "Y_train = to_categorical(y_train, num_classes=10).T\n",
        "Y_test  = to_categorical(y_test, num_classes=10).T\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim  = X_train_flat.shape[0]  # 784\n",
        "hidden_dim = 128\n",
        "output_dim = 10\n"
      ],
      "metadata": {
        "id": "VOQJY-XHKjN6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Training using Traditional Gradient Descent --------------------\n",
        "print(\"Training with traditional gradient descent:\")\n",
        "nn_gd = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "start_time = time.time()\n",
        "nn_gd.train(X_train_flat, Y_train, iterations=2000, learning_rate=0.01, verbose=True)\n",
        "print(f\"Traditional GD training time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate on test set\n",
        "predictions = nn_gd.predict(X_test_flat)\n",
        "accuracy = np.mean(predictions == np.argmax(Y_test, axis=0)) * 100\n",
        "print(f\"Test Accuracy (Traditional GD): {accuracy:.2f}%\\n\")\n",
        "\n",
        "# -------------------- Training using Minibatch Gradient Descent --------------------\n",
        "print(\"Training with minibatch gradient descent:\")\n",
        "nn_mb = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "start_time = time.time()\n",
        "nn_mb.train_minibatch(X_train_flat, Y_train, batch_size=256, iterations=2000, learning_rate=0.01, verbose=True)\n",
        "print(f\"Minibatch GD training time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate on test set\n",
        "predictions = nn_mb.predict(X_test_flat)\n",
        "accuracy = np.mean(predictions == np.argmax(Y_test, axis=0)) * 100\n",
        "print(f\"Test Accuracy (Minibatch GD): {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix708VsJKjQY",
        "outputId": "bb4f7053-acd5-4757-9dbc-89ba760e109a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with traditional gradient descent:\n",
            "Iteration 0: cost = 2.3028\n",
            "Iteration 100: cost = 2.2905\n",
            "Iteration 200: cost = 2.2682\n",
            "Iteration 300: cost = 2.2213\n",
            "Iteration 400: cost = 2.1266\n",
            "Iteration 500: cost = 1.9597\n",
            "Iteration 600: cost = 1.7192\n",
            "Iteration 700: cost = 1.4532\n",
            "Iteration 800: cost = 1.2258\n",
            "Iteration 900: cost = 1.0547\n",
            "Iteration 1000: cost = 0.9285\n",
            "Iteration 1100: cost = 0.8335\n",
            "Iteration 1200: cost = 0.7603\n",
            "Iteration 1300: cost = 0.7026\n",
            "Iteration 1400: cost = 0.6562\n",
            "Iteration 1500: cost = 0.6181\n",
            "Iteration 1600: cost = 0.5865\n",
            "Iteration 1700: cost = 0.5599\n",
            "Iteration 1800: cost = 0.5371\n",
            "Iteration 1900: cost = 0.5175\n",
            "Traditional GD training time: 2530.47 seconds\n",
            "Test Accuracy (Traditional GD): 87.56%\n",
            "\n",
            "Training with minibatch gradient descent:\n",
            "Minibatch Iteration 0: cost = 2.2607\n",
            "Minibatch Iteration 100: cost = 0.2183\n",
            "Minibatch Iteration 200: cost = 0.1244\n",
            "Minibatch Iteration 300: cost = 0.0796\n",
            "Minibatch Iteration 400: cost = 0.0489\n",
            "Minibatch Iteration 500: cost = 0.0166\n",
            "Minibatch Iteration 600: cost = 0.0394\n",
            "Minibatch Iteration 700: cost = 0.0115\n",
            "Minibatch Iteration 800: cost = 0.0312\n",
            "Minibatch Iteration 900: cost = 0.0199\n",
            "Minibatch Iteration 1000: cost = 0.0412\n",
            "Minibatch Iteration 1100: cost = 0.0216\n",
            "Minibatch Iteration 1200: cost = 0.0089\n",
            "Minibatch Iteration 1300: cost = 0.0108\n",
            "Minibatch Iteration 1400: cost = 0.0287\n",
            "Minibatch Iteration 1500: cost = 0.0083\n",
            "Minibatch Iteration 1600: cost = 0.0058\n",
            "Minibatch Iteration 1700: cost = 0.0077\n",
            "Minibatch Iteration 1800: cost = 0.0051\n",
            "Minibatch Iteration 1900: cost = 0.0056\n",
            "Minibatch GD training time: 3335.30 seconds\n",
            "Test Accuracy (Minibatch GD): 97.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used minibatch gradient descent because it updates the model more frequently by processing smaller subsets (batches) of the data at a time. Although it took a bit longer to train overall, these more frequent updates helped the network converge better, resulting in higher test accuracy (97.94%) compared to traditional gradient descent, which uses the entire dataset for each update."
      ],
      "metadata": {
        "id": "DseVO34rzNHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "kdFJOaLy0JwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "For learning PyTorch and implementing a 2-layer Neural Network (NN), I explored the following resources:\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html – This tutorial provides hands-on examples of tensors, forward and backward passes, and basic neural network operations. It helped me understand the core mechanics of PyTorch, including automatic differentiation and gradient computation, which are crucial for training a neural network.\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html – This guide focuses specifically on constructing neural networks in PyTorch using torch.nn.Module. It was essential for learning how to define a 2-layer NN efficiently and track computations using built-in layers and optimizers.\n",
        "\n",
        "These resources were necessary to grasp fundamental PyTorch operations, model construction, and the backpropagation process, ensuring an effective implementation of my neural network."
      ],
      "metadata": {
        "id": "HEULDGls0PCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "B9qEKPE70chv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization:"
      ],
      "metadata": {
        "id": "NeVr4Huj1FXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "\n",
        "data, target = next(iter(test_loader))\n",
        "data = data[0]\n",
        "target = target[0]\n",
        "# Visualize the image\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(data.squeeze().numpy(), cmap='gray')\n",
        "plt.title(f'Label: {target.item()}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "UD6OBox7KjS4",
        "outputId": "ba781e92-f848-42b1-a048-23d9709d5898"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 43.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.55MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.94MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "a9_-QgJg1z3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Function:\n",
        "\n",
        "ReLU (Rectified Linear Unit): Applied after the first linear layer to introduce non-linearity.\n",
        "Hyperparameters:\n",
        "\n",
        "Input Size: 28×28 (flattened to 784).\n",
        "Hidden Layer: 512 neurons.\n",
        "Output Layer: 10 neurons (for MNIST digits 0-9).\n",
        "Loss Function: CrossEntropyLoss.\n",
        "Optimizer: Adam with a learning rate of 0.001.\n",
        "Epochs: 5.\n",
        "Batch Size: 64."
      ],
      "metadata": {
        "id": "jDOMNm-X3VxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "class NeuralNetworkPytorch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# Initialization\n",
        "model = NeuralNetworkPytorch().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data.to(device)).to(device)\n",
        "        loss = criterion(outputs, targets.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Xq3yRlKjVW",
        "outputId": "e98d084b-b265-43e4-faaf-9534a0d62cd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:10<00:00, 906kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 58.6kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Epoch 1/5, Batch 0/938, Loss: 2.3578\n",
            "Epoch 1/5, Batch 100/938, Loss: 0.3925\n",
            "Epoch 1/5, Batch 200/938, Loss: 0.1835\n",
            "Epoch 1/5, Batch 300/938, Loss: 0.2475\n",
            "Epoch 1/5, Batch 400/938, Loss: 0.0633\n",
            "Epoch 1/5, Batch 500/938, Loss: 0.0530\n",
            "Epoch 1/5, Batch 600/938, Loss: 0.1044\n",
            "Epoch 1/5, Batch 700/938, Loss: 0.0791\n",
            "Epoch 1/5, Batch 800/938, Loss: 0.1424\n",
            "Epoch 1/5, Batch 900/938, Loss: 0.0812\n",
            "Epoch 2/5, Batch 0/938, Loss: 0.0217\n",
            "Epoch 2/5, Batch 100/938, Loss: 0.0400\n",
            "Epoch 2/5, Batch 200/938, Loss: 0.1218\n",
            "Epoch 2/5, Batch 300/938, Loss: 0.0874\n",
            "Epoch 2/5, Batch 400/938, Loss: 0.1286\n",
            "Epoch 2/5, Batch 500/938, Loss: 0.1520\n",
            "Epoch 2/5, Batch 600/938, Loss: 0.1882\n",
            "Epoch 2/5, Batch 700/938, Loss: 0.0620\n",
            "Epoch 2/5, Batch 800/938, Loss: 0.1669\n",
            "Epoch 2/5, Batch 900/938, Loss: 0.0244\n",
            "Epoch 3/5, Batch 0/938, Loss: 0.0267\n",
            "Epoch 3/5, Batch 100/938, Loss: 0.0195\n",
            "Epoch 3/5, Batch 200/938, Loss: 0.0102\n",
            "Epoch 3/5, Batch 300/938, Loss: 0.0409\n",
            "Epoch 3/5, Batch 400/938, Loss: 0.0404\n",
            "Epoch 3/5, Batch 500/938, Loss: 0.0164\n",
            "Epoch 3/5, Batch 600/938, Loss: 0.0259\n",
            "Epoch 3/5, Batch 700/938, Loss: 0.0149\n",
            "Epoch 3/5, Batch 800/938, Loss: 0.0836\n",
            "Epoch 3/5, Batch 900/938, Loss: 0.0367\n",
            "Epoch 4/5, Batch 0/938, Loss: 0.0172\n",
            "Epoch 4/5, Batch 100/938, Loss: 0.0135\n",
            "Epoch 4/5, Batch 200/938, Loss: 0.0165\n",
            "Epoch 4/5, Batch 300/938, Loss: 0.0031\n",
            "Epoch 4/5, Batch 400/938, Loss: 0.0280\n",
            "Epoch 4/5, Batch 500/938, Loss: 0.0072\n",
            "Epoch 4/5, Batch 600/938, Loss: 0.0175\n",
            "Epoch 4/5, Batch 700/938, Loss: 0.0190\n",
            "Epoch 4/5, Batch 800/938, Loss: 0.0138\n",
            "Epoch 4/5, Batch 900/938, Loss: 0.1915\n",
            "Epoch 5/5, Batch 0/938, Loss: 0.0345\n",
            "Epoch 5/5, Batch 100/938, Loss: 0.0986\n",
            "Epoch 5/5, Batch 200/938, Loss: 0.0306\n",
            "Epoch 5/5, Batch 300/938, Loss: 0.0118\n",
            "Epoch 5/5, Batch 400/938, Loss: 0.0262\n",
            "Epoch 5/5, Batch 500/938, Loss: 0.0010\n",
            "Epoch 5/5, Batch 600/938, Loss: 0.0130\n",
            "Epoch 5/5, Batch 700/938, Loss: 0.1695\n",
            "Epoch 5/5, Batch 800/938, Loss: 0.0138\n",
            "Epoch 5/5, Batch 900/938, Loss: 0.0469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "Normalization improves model performance by stabilizing training, leading to faster convergence and better generalization. When inputs are normalized, gradients flow more efficiently, reducing issues like vanishing or exploding gradients. This results in a lower loss and higher accuracy compared to unnormalized inputs. In my experiments, the normalized model trained faster and achieved better accuracy (~98% vs. ~97% on MNIST)."
      ],
      "metadata": {
        "id": "6-JdflzS3tiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, targets in test_loader:\n",
        "        outputs = model(data.to(device)).to(device)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets.to(device)).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy on the test set: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCoXramF11IP",
        "outputId": "9c44db1d-e607-4a8b-b041-a53c035b90b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 97.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "at0rJ6l438Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the hyperparameters based on standard practices for training a simple neural network on MNIST. The learning rate (0.001) is a common choice for Adam optimizer, balancing convergence speed and stability. The batch size (64) is a good trade-off between computational efficiency and gradient estimation accuracy. The number of epochs (5) is sufficient for MNIST without overfitting.\n",
        "\n",
        "I used ReLU activation to introduce non-linearity and prevent vanishing gradients. I did not explicitly use regularization (like dropout or weight decay) because MNIST is relatively simple, and the model's complexity is low. However, normalization in preprocessing helps with training stability.\n",
        "\n",
        "I used the Adam optimizer because it adapts learning rates per parameter and generally performs well across different architectures without much tuning. It also converges faster than vanilla SGD."
      ],
      "metadata": {
        "id": "xo4EgbPb4Em6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4"
      ],
      "metadata": {
        "id": "xp0rOPZ-59Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "t0 = time.time()\n",
        "train_samples = 60000\n",
        "\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=train_samples, test_size=10000\n",
        ")\n",
        "\n",
        "\n",
        "# Train one-vs-all logistic regression classifier for each digit\n",
        "classifiers = []\n",
        "for digit in range(10):\n",
        "    y_train_digit = (y_train == str(digit)).astype(int)\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train_digit)\n",
        "    classifiers.append(clf)\n",
        "\n",
        "# Predict using all classifiers\n",
        "predictions = np.vstack([clf.predict(X_test) for clf in classifiers])\n",
        "\n",
        "# Select the class with the highest probability as the predicted class\n",
        "y_pred = np.argmax(predictions, axis=0)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test.astype(int), y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49FrftRN11LE",
        "outputId": "2c499fed-171d-41b2-f07b-861a2f90aa2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network (97.93% accuracy) outperforms logistic regression (85.32%) because it can learn complex, non-linear patterns and extract hierarchical features from the MNIST dataset. Logistic regression, being a linear model, struggles with capturing intricate spatial relationships in images. Neural networks use activation functions and multiple layers, allowing them to better classify handwritten digits, whereas logistic regression is limited by its linear decision boundaries."
      ],
      "metadata": {
        "id": "0gCviQ1l7Ui_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtNZ8ky25j6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ft9sUJY75FIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}